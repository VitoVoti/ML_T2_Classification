{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tarea 2 Máquinas de Aprendizaje</center>\n",
    "\n",
    "<center>\n",
    "Patricio Horth M.<br>\n",
    "Víctor Zúñiga M.<br>\n",
    "\n",
    "2 de Noviembre de 2017\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Análisis de Opiniones sobre Películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de sentimiento (o minerı́a de opiniones) se refiere al proceso de extraer información acerca de\n",
    "la actitud que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con\n",
    "respecto a un tópico o contexto de comunicación. Uno de los casos más estudiados corresponde a determinar\n",
    "la polaridad de un trozo de texto, es decir, clasificar una determinada evaluación escrita (ó review ), en que\n",
    "una persona manifiesta una opinión, como positiva, negativa o neutral. La dificultad de este problema radica\n",
    "en el carácter altamente ambiguo e informal del lenguaje que utilizan naturalmente las personas ası́ como el\n",
    "manejo de negaciones, sarcasmo y abreviaciones en una frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a . Construya un dataframe con los datos a analizar descargando los datos desde la URL local. Determine cuántos registros de cada clase contiene el conjunto de entrenamiento y cuántos el conjunto de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Se contruye el data frame \n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.request.urlretrieve(train_data_url, \"train_data_item2.csv\")\n",
    "test_data_f = urllib.request.urlretrieve(test_data_url, \"test_data_item2.csv\")\n",
    "ftr = open(\"train_data_item2.csv\", \"r\")\n",
    "fts = open(\"test_data_item2.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este dataset tenemos que la cantidad de clases estará dada por la columna \"Sentiment\" del dataset, donde existen dos valores posibles $+1$ para comentarios positivos y $-1$ para comentarios negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>screenwriter dan schneider and director shawn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>home alone goes hollywood , a funny premise un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>seldom has a movie so closely matched the spir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>less dizzying than just dizzy , the jaunt is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>an ultra-low-budget indie debut that smacks mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0         -1  screenwriter dan schneider and director shawn ...\n",
       "1         -1  home alone goes hollywood , a funny premise un...\n",
       "2          1  seldom has a movie so closely matched the spir...\n",
       "3         -1  less dizzying than just dizzy , the jaunt is p...\n",
       "4         -1  an ultra-low-budget indie debut that smacks mo..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pr otro lado, la cantidad de registros totales que incluye cada data set será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Conjunto de entrenamiento================\n",
      "La cantidad de elementos que tienen la clase +1 es:  1770\n",
      "La cantidad de elementos que tienen la clase -1 es:  1784\n",
      "Total: 3554 registros\n",
      "=========================================================\n",
      "================Conjunto de prueba=======================\n",
      "La cantidad de elementos que tienen la clase +1 es:  1751\n",
      "La cantidad de elementos que tienen la clase -1 es:  1803\n",
      "Total: 3554 registros\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "count_train_df = pd.value_counts(train_df['Sentiment'].values, sort=False)\n",
    "count_test_df = pd.value_counts(test_df['Sentiment'].values, sort=False)\n",
    "print(\"================Conjunto de entrenamiento================\")\n",
    "print(\"La cantidad de elementos que tienen la clase +1 es: \",count_train_df[1])\n",
    "print(\"La cantidad de elementos que tienen la clase -1 es: \",count_train_df[-1])\n",
    "print(\"Total:\",train_df.shape[0], \"registros\")\n",
    "print(\"=========================================================\")\n",
    "print(\"================Conjunto de prueba=======================\")\n",
    "print(\"La cantidad de elementos que tienen la clase +1 es: \",count_test_df[1])\n",
    "print(\"La cantidad de elementos que tienen la clase -1 es: \",count_test_df[-1])\n",
    "print(\"Total:\",test_df.shape[0], \"registros\")\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar en nuestros dataset, existe un pequeño desbalance en la cantidad de registros que tiene cada clase, ya que para ambos casos (entrenamiento y test) la clase $-1$ tiene más registros, pero como la diferencia es practicamente despreciable supondremos que no representará un problema a la hora de clasificar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Construya una función, denominada word extractor, que devuelva una lista de las palabras contenidas en un determinado un trozo de texto. Incorpore en su función las operaciones de lower-casing y stemming. Pruebe la función con las frases sugeridas en el código, invente otras similares y comente. Compare con los resultados obtenidos si no se hace stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se implementa la funcion word_extractor, la cual hace un preprocesamiento de las frases contenidas en la columna Text del dataset, la cual quita las stop words y aplica stemming mediante el algoritmo de Porter. Tambien  se encuentra la implementacion de la funcion word_extractor_simple que a diferencia de la anteriorel unico preporcesamiento que realiza es simplemente quitar las stop words de cada frase.\n",
    "Al aplicar stemming a una palabra lo que se hace es encontrar su raiz, es decir, se busca que la forma de las palabras no penalice la frecuencia de ésta ya que, una palabra puede estar conjugada en cualquier (género, número, persona, etc.) y solo se considerará (en muchas ocasiones) como un solo término."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text,include_stopwords = False):\n",
    "    ps = PorterStemmer()\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)\n",
    "    words = \"\"\n",
    "    wordtokens = [ ps.stem(word.lower()) for word in word_tokenize(text) ]\n",
    "    if include_stopwords == False:\n",
    "        commonwords = stopwords.words('english')\n",
    "        commonwords.remove(\"not\")\n",
    "        for word in wordtokens:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "    else:\n",
    "        for word in wordtokens:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "def word_extractor_simple(text):\n",
    "    ps = PorterStemmer()\n",
    "    #Se eliminan las \"palabras vacias\"\n",
    "    commonwords = stopwords.words('english')\n",
    "    commonwords.remove(\"not\")\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() for word in word_tokenize(text) ]\n",
    "    for word in wordtokens:\n",
    "        #Se dejan solo las palabras relevantes\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A contuacion se probara el uso de ambas funciones con algunas palabras de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Pruebas usando Porter Stemmer \n",
      "===============================\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " not love eat cake\n",
      " n't love eat cake\n",
      " awak late\n",
      " awok late\n",
      " awak late\n",
      " would awaken\n",
      "===============================\n",
      "Pruebas solo quitando stop words\n",
      "===============================\n",
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " not love eating cake\n",
      " n't love eating cake\n",
      " awoke late\n",
      " awaking late\n",
      " would awaken\n"
     ]
    }
   ],
   "source": [
    "print(\"===============================\")\n",
    "print(\"Pruebas usando Porter Stemmer \")\n",
    "print(\"===============================\")\n",
    "print(word_extractor(\"I love to eat cake\",False))\n",
    "print(word_extractor(\"I love eating cake\",False))\n",
    "print(word_extractor(\"I loved eating the cake\",False))\n",
    "print(word_extractor(\"I do not love eating cake\",False))\n",
    "print(word_extractor(\"I don't love eating cake\",False))\n",
    "print(word_extractor(\"You awaked late\",False))\n",
    "print(word_extractor(\"You awoke late\",False))\n",
    "print(word_extractor(\"You have been awaking late\",False))\n",
    "print(word_extractor(\"I would have awaken\",False))\n",
    "print(\"===============================\")\n",
    "print(\"Pruebas solo quitando stop words\")\n",
    "print(\"===============================\")\n",
    "print(word_extractor_simple(\"I love to eat cake\"))\n",
    "print(word_extractor_simple(\"I love eating cake\"))\n",
    "print(word_extractor_simple(\"I loved eating the cake\"))\n",
    "print(word_extractor_simple(\"I do not love eating cake\"))\n",
    "print(word_extractor_simple(\"I don't love eating cake\"))\n",
    "print(word_extractor_simple(\"You awoke late\"))\n",
    "print(word_extractor_simple(\"You have been awaking late\"))\n",
    "print(word_extractor_simple(\"I would have awaken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como podemos apreciar la diferencia es evidente ya que al usar en el pre procesamiento Porter Stemmer al quedar solo la raiz de la palabra, la frecuencia de esta aumenta, mientras que al solo quitar las stop words las conjugaciones de cada palabra se mantienen y serán consideradas como una palabra diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Construya una función, denominada word extractor2, análoga a la función anterior, pero que lematice las palabras en vez de hacer stemming. Pruebe la función con las frases sugeridas en el código anterior y discuta las diferencias que observa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se presentara la implementacion de la funcion word_extractor2 para realizar el preprocesamiento de las palabras contenidas en el dataset. La diferencia de esta funcion con las demás radica en que se sustituira el uso de Stemming por un algoritmo de lematización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\horth_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correción aportada por Francisco Casas en el moodle del curso\n",
    "# https://moodle.inf.utfsm.cl/mod/forum/discuss.php?d=31466\n",
    "\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('JJ'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('RB') or tag == \"WRB\":\n",
    "        return 'r'\n",
    "    elif tag.startswith('NN') or tag.startswith(\"WP\"):\n",
    "        return 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_words(lemmatizer,words):\n",
    "    tagged = pos_tag(words)\n",
    "    lemmas = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_tag(tag)\n",
    "        if wntag is None:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word,pos=wntag)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_extractor2(text,include_stopwords = False):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)\n",
    "    words = \"\"\n",
    "    wordtokens = lemmatize_words(wordlemmatizer,word_tokenize(text.lower()))\n",
    "    if include_stopwords == False:\n",
    "        commonwords = stopwords.words('english')\n",
    "        commonwords.remove(\"not\")\n",
    "        for word in wordtokens:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "    else:\n",
    "        for word in wordtokens:\n",
    "            words+=\" \"+word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es un proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), se encuentra el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra. Es decir, el lema de una palabra es la palabra que nos encontraríamos como entrada en un diccionario tradicional: singular para sustantivos, masculino singular para adjetivos, infinitivo para verbos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Pruebas usando lematización\n",
      "===============================\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " not love eat cake\n",
      " n't love eat cake\n",
      " awake late\n",
      " awake late\n",
      " i would have awaken\n"
     ]
    }
   ],
   "source": [
    "print(\"===============================\")\n",
    "print(\"Pruebas usando lematización\")\n",
    "print(\"===============================\")\n",
    "print(word_extractor2(\"I love to eat cake\",False))\n",
    "print(word_extractor2(\"I love eating cake\",False))\n",
    "print(word_extractor2(\"I loved eating the cake\",False))\n",
    "print(word_extractor2(\"I do not love eating cake\",False))\n",
    "print(word_extractor2(\"I don't love eating cake\",False))\n",
    "print(word_extractor2(\"You awoke late\",False))\n",
    "print(word_extractor2(\"You have been awaking late\",False))\n",
    "print(word_extractor2(\"I would have awaken\",True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar a diferencia del uso de un algoritmo de stemming que realiza cortes alfinal y en ocaciones al inicio de una palabra en busca de su raiz, donde este corte \"indiscriminado\" puede resultar ser correcto en ciertas ocaciones pero no siempre, la lematizacion hace un análisis morfologico de la palabra entregando de forma correcta el lemma de la palabra, es decir la base para todas las formas flexionadas de esta. Además la raíz o stemm puede ser la misma para las formas flexionadas de diferentes lemas, lo que proporciona ruido a nuestros resultados de búsqueda.\n",
    "\n",
    "Comparando los resultados obtenidos en este punto con el anterior podemos verificar que el uso de la lematizacion es mas preciso que el uso de algún stemmer, por ejemplo para las conjugaciones del verbo awake el uso de stemming entrego dos raices distintas, mientras que la lematizacion entrego para todas sus conjugaciones el mismo lemma.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Utilizando la función CountVectorizer de la librería sklearn y de acuerdo a las directrices mencionadasen la introducción, genere una representación vectorial del texto de entrenamiento y del conjunto que usaremos para realizar pruebas. Explore el vocabulario utilizado y determine cuáles son las palabras más frecuentes en el conjunto de entrenamiento y pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tabulate  import tabulate\n",
    "\n",
    "def vect_rep(train_df, test_df, v = \"porter\", include_stopwords = False, v1 = \"train\"):\n",
    "    if v == \"porter\":\n",
    "        texts_train = [word_extractor(text,include_stopwords) for text in train_df.Text]\n",
    "        texts_test = [word_extractor(text,include_stopwords) for text in test_df.Text]\n",
    "    if v == \"lem\":\n",
    "        texts_train = [word_extractor2(text,include_stopwords) for text in train_df.Text]\n",
    "        texts_test = [word_extractor2(text,include_stopwords) for text in test_df.Text]\n",
    "    if v == \"simple\":\n",
    "        texts_train = [word_extractor_simple(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor_simple(text) for text in test_df.Text]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    if v1 == \"train\":\n",
    "        features_train = vectorizer.transform(texts_train)\n",
    "        labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "        dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "        features = features_train\n",
    "        labels = labels_train\n",
    "    if v1 == \"test\":\n",
    "        features_test = vectorizer.transform(texts_test)\n",
    "        labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "        dist=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "        features = features_test\n",
    "        labels = labels_test\n",
    "    #Se modifica el codigo original para guardar cada tag y su frecuencia \n",
    "    frec_tag = []\n",
    "    for tag, count in zip(vocab, dist):\n",
    "        frec_tag.append([count, tag])\n",
    "    return frec_tag,features,labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_more_frequent(l,n):\n",
    "    # Se ordenan de manera descendente\n",
    "    sorted_tags = sorted(l, reverse=True)\n",
    "    # Se guardan las n palabras mas frecuentes\n",
    "    elements = []\n",
    "    for i in range(n):\n",
    "        elements.append(sorted_tags[i])\n",
    "    return elements\n",
    "\n",
    "def tab(a,b,c,n):\n",
    "    aa = n_more_frequent(a,n)\n",
    "    bb = n_more_frequent(b,n)\n",
    "    cc = n_more_frequent(c,n)\n",
    "    from tabulate import tabulate\n",
    "    tab = []\n",
    "    for i  in range(len(aa)):\n",
    "        tab.append([aa[i],bb[i],cc[i]])\n",
    "    return tab   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,features_train_porter,labels_train_porter = vect_rep(train_df, test_df, \"porter\",False,\"train\")\n",
    "b,features_train_lem,labels_train_lem = vect_rep(train_df, test_df, \"lem\",False,\"train\")\n",
    "e,features_train_simple,labels_train_simple = vect_rep(train_df, test_df, \"simple\",\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c,features_test_porter,labels_test_porter = vect_rep(train_df, test_df, \"porter\",False,\"test\")\n",
    "d,features_test_lem,labels_test_lem = vect_rep(train_df, test_df, \"lem\",False,\"test\")\n",
    "f,features_test_simple,labels_test_simple = vect_rep(train_df, test_df, \"simple\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 15 palabras mas frecuentes para el conjunto de entrenamiento\n",
      "PorterStemmer     wordlemmatizer      Simple\n",
      "----------------  ------------------  -------------------\n",
      "[575, 'film']     [574, 'film']       [519, 'film']\n",
      "[477, 'movi']     [481, 'movie']      [421, 'movie']\n",
      "[425, 'thi']      [266, 'not']        [266, 'not']\n",
      "[269, 'not']      [264, 'make']       [244, 'like']\n",
      "[268, 'like']     [254, 'like']       [238, 'one']\n",
      "[245, 'one']      [246, 'one']        [158, 'story']\n",
      "[224, 'ha']       [176, 'story']      [143, 'even']\n",
      "[207, 'make']     [169, 'good']       [140, 'good']\n",
      "[179, 'hi']       [163, 'character']  [135, 'comedy']\n",
      "[176, 'stori']    [145, 'comedy']     [128, 'much']\n",
      "[161, 'charact']  [143, 'time']       [113, 'characters']\n",
      "[148, 'time']     [143, 'even']       [111, 'well']\n",
      "[144, 'good']     [129, 'work']       [107, 'time']\n",
      "[144, 'even']     [128, 'much']       [103, 'make']\n",
      "[142, 'comedi']   [127, 'well']       [103, 'little']\n",
      "\n",
      "\n",
      "Las 15 palabras mas frecuentes para el conjunto de prueba\n",
      "PorterStemmer     wordlemmatizer      Simple\n",
      "----------------  ------------------  -------------------\n",
      "[572, 'film']     [563, 'film']       [519, 'film']\n",
      "[530, 'movi']     [541, 'movie']      [421, 'movie']\n",
      "[514, 'thi']      [278, 'not']        [266, 'not']\n",
      "[281, 'not']      [250, 'one']        [244, 'like']\n",
      "[250, 'one']      [238, 'make']       [238, 'one']\n",
      "[248, 'like']     [235, 'like']       [158, 'story']\n",
      "[238, 'ha']       [197, 'story']      [143, 'even']\n",
      "[196, 'stori']    [182, 'good']       [140, 'good']\n",
      "[187, 'make']     [175, 'character']  [135, 'comedy']\n",
      "[184, 'hi']       [165, 'time']       [128, 'much']\n",
      "[171, 'charact']  [134, 'comedy']     [113, 'characters']\n",
      "[163, 'time']     [130, 'even']       [111, 'well']\n",
      "[134, 'good']     [124, 'well']       [107, 'time']\n",
      "[132, 'doe']      [124, 'much']       [103, 'make']\n",
      "[131, 'even']     [124, 'go']         [103, 'little']\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "tab_train = tab(a,b,e,n)\n",
    "tab_test = tab(c,d,f,n)\n",
    "\n",
    "print(\"Las\",n,\"palabras mas frecuentes para el conjunto de entrenamiento\")\n",
    "print(tabulate(tab_train, headers=['PorterStemmer', 'wordlemmatizer', 'Simple']))\n",
    "print(\"\\n\")\n",
    "print(\"Las\",n,\"palabras mas frecuentes para el conjunto de prueba\")\n",
    "print(tabulate(tab_test, headers=['PorterStemmer', 'wordlemmatizer','Simple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos para ambos datasets el uso del stemmer y la lematizacion entregan resultados muy similares, sin embargo podemos ver que el uso de la lematizaciónentrega algunas palabras que proveen un poco mas de información. Por otro lado solo quitar las palabras vacias se obtienen un resultado muy similar al uso de la lematizacion pero la frecuencia de cada palabra entregada es bastante menor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_error(yhat, ytest):\n",
    "    error_total = 0\n",
    "    for i in range(1, len(yhat)):\n",
    "        if yhat[i] != ytest[i]:\n",
    "            error_total = error_total + 1\n",
    "    return error_total / len(yhat)\n",
    "\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    error = get_error(model.predict(xt),yt)\n",
    "    print(\"Training Accuracy %s: %f\"%(text,acc_tr))\n",
    "    print(\"Test Accuracy %s: %f\"%(text,acc_test))\n",
    "    print(\"Error:\", error)\n",
    "    print( \"Detailed Analysis Testing Results ...\")\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "\n",
    "Tenemos que las metricas que entrega la funcion classification report son un resumen de texto de:\n",
    "    \n",
    "$Precision:$  La precisión es el ratio entre las observaciones positivas predichas correctamente y las observaciones positivas predichas totales.Su calculo esta dado por: \n",
    "\n",
    "$\\displaystyle \\frac{\\text{verdadero postivo}}{\\text{verdaro positivo} + \\text{falso positivo}}$\n",
    "    \n",
    "$Recall:$ Esta metrica representa el ratio de observaciones positivas predichas correctamente del total de observaciones en la clase actual. Su calculo esta dado por: \n",
    "\n",
    "$\\displaystyle \\frac{\\text{verdadero postivo}}{\\text{verdaro positivo} + \\text{falso negativo}}$\n",
    "\n",
    "$F1 score:$ Se puede interpretar como un promedio ponderado de la precisión y el recall, donde alcanza su mejor valor en 1 y el peor puntaje en 0. La contribución relativa de la precisión y el recall son iguales. Su calculo esta dado por:\n",
    "\n",
    " $\\displaystyle F1 = 2 * \\frac{precision * recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Construya una función que entrene/ajuste un clasificador Bayesiano Ingenuo (Binario) (las caracteristicas no nulas serán tratadas como 1) y mida el error de predicción obtenido sobre los datos de entrenamiento y pruebas. \n",
    "#### Utilice esta función con las características extraídas en el punto (d). Mida el efecto de filtrar stopwords y de eliminar este paso de pre-procesamiento típico. \n",
    "#### Determine además, qué representación obtiene un mejor resultado: si aquella obtenida vía lematización o aquella obtenida vía stemming. \n",
    "#### Finalmente, tome un subconjunto aleatorio de los textos de prueba y analice las predicciones del modelo (explore las predicciones, así como las probabilidades que el clasificador asigna a cada clase).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import random\n",
    "\n",
    "#Error de clasificacion\n",
    "\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "def get_model_metrics(features_train,labels_train, features_test, labels_test):\n",
    "    model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test) \n",
    "    #Retorna la probabilidad de pertenencia para cada clase del modelo usando el vector de test.\n",
    "    test_pred = model.predict_proba(features_test)\n",
    "    #Selecciona de forma aleatorea la posicion de 15 elementos en el vector test_pred.\n",
    "    spl = random.sample(range(len(test_pred)),10)\n",
    "    data = []\n",
    "    for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "        data.append([sentiment, text[0:80]])\n",
    "    print(tabulate(data, headers=['Sentiment', 'Text'],tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftag_1,features_train_porter2,labels_train_porter2 = vect_rep(train_df, test_df, \"porter\",True,\"train\")\n",
    "ftag_2,features_test_porter2,labels_test_porter2 = vect_rep(train_df, test_df, \"porter\",True,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftag_3,features_train_lem2,labels_train_lem2 = vect_rep(train_df, test_df, \"lem\",True,\"train\")\n",
    "ftag_4,features_test_lem2,labels_test_lem2 = vect_rep(train_df, test_df, \"lem\",True,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando porter\n",
      "-------------------------------------------------------------------\n",
      "Training Accuracy BernoulliNB: 0.942600\n",
      "Test Accuracy BernoulliNB: 0.748100\n",
      "Error: 0.2518289251547552\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Sentiment                           │ Text                                                                             │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ [ 0.77443158  0.22556842]           │ i found it slow , predictable and not very amusing .                             │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.91921754  0.08078246]           │ witless , pointless , tasteless and idiotic .                                    │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [  9.99977299e-01   2.27013019e-05] │ it's tough to tell which is in more abundant supply in this woefully hackneyed m │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.96767872  0.03232128]           │ succeeds where its recent predecessor miserably fails because it demands that yo │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.7973592  0.2026408]             │ the film starts out as competent but unremarkable . . . and gradually grows into │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.15863218  0.84136782]           │ most of crush is a clever and captivating romantic comedy with a welcome pinch o │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.9714795  0.0285205]             │ it's one thing to read about or rail against the ongoing - and unprecedented - c │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.95200651  0.04799349]           │ a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing  │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.84258756  0.15741244]           │ from its nauseating spinning credits sequence to a very talented but underutiliz │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.0099454  0.9900546]             │ 'film aficionados cannot help but love cinema paradiso , whether the original ve │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"usando porter\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "get_model_metrics(features_train_porter, labels_train_porter, features_test_porter,labels_test_porter)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando porter sin quitar las stopwords\n",
      "-------------------------------------------------------------------\n",
      "Training Accuracy BernoulliNB: 0.938661\n",
      "Test Accuracy BernoulliNB: 0.762736\n",
      "Error: 0.23719752391671356\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.77      0.76      0.77      1803\n",
      "          -       0.76      0.76      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Sentiment                           │ Text                                                                             │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ [ 0.01041238  0.98958762]           │ a smart and funny , albeit sometimes superficial , cautionary tale of a technolo │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.9912011  0.0087989]             │ the film is so packed with subplots involving the various silbersteins that it f │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.01094298  0.98905702]           │ it is inspirational in characterizing how people from such diverse cultures shar │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [  9.99007753e-01   9.92247465e-04] │ in exactly 89 minutes , most of which passed as slowly as if i'd been sitting na │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.61001029  0.38998971]           │ an odd , haphazard , and inconsequential romantic comedy .                       │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.32084082  0.67915918]           │ twenty years later , e . t . is still a cinematic touchstone .                   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.25833268  0.74166732]           │ a close-to-solid espionage thriller with the misfortune of being released a few  │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.9033521  0.0966479]             │ a superfluous sequel . . . plagued by that old familiar feeling of 'let's get th │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.99211119  0.00788881]           │ this is the type of movie best enjoyed by frat boys and college kids while sucki │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.33673438  0.66326562]           │ confessions isn't always coherent , but it's sharply comic and surprisingly touc │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(\"usando porter sin quitar las stopwords\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "get_model_metrics(features_train_porter2, labels_train_porter2, features_test_porter2,labels_test_porter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando una comparativa podemos ver que el desempeño en general del modelo aumenta si no quitamos las stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando lematizacion\n",
      "-------------------------------------------------------------------\n",
      "Training Accuracy BernoulliNB: 0.951604\n",
      "Test Accuracy BernoulliNB: 0.742753\n",
      "Error: 0.25717501406865506\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.74      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "╒═══════════════════════════╤══════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Sentiment                 │ Text                                                                             │\n",
      "╞═══════════════════════════╪══════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ [ 0.44054005  0.55945995] │ if you can push on through the slow spots , you'll be rewarded with some fine ac │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.60122062  0.39877938] │ one can't deny its seriousness and quality .                                     │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.71301002  0.28698998] │ a noble failure .                                                                │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.01497684  0.98502316] │ tadpole is a sophisticated , funny and good-natured treat , slight but a pleasur │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.61622347  0.38377653] │ sit through this one , and you won't need a magic watch to stop time ; your dvd  │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.15324725  0.84675275] │ a reasonably entertaining sequel to 1994's surprise family hit that may strain a │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.92035134  0.07964866] │ these spiders can outrun a motorcycle and wrap a person in a sticky cocoon in se │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.78597372  0.21402628] │ too bland and fustily tasteful to be truly prurient .                            │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.69233349  0.30766651] │ in the end , there isn't much to it .                                            │\n",
      "├───────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.3755968  0.6244032]   │ the gifted crudup has the perfect face to play a handsome blank yearning to find │\n",
      "╘═══════════════════════════╧══════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(\"usando lematizacion\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "get_model_metrics(features_train_lem, labels_train_lem, features_test_lem,labels_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando lematizacion sin quitar las stopwords\n",
      "-------------------------------------------------------------------\n",
      "Training Accuracy BernoulliNB: 0.946539\n",
      "Test Accuracy BernoulliNB: 0.750070\n",
      "Error: 0.24985931344963422\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.75      0.75      1803\n",
      "          -       0.75      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "╒═════════════════════════════════════╤══════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Sentiment                           │ Text                                                                             │\n",
      "╞═════════════════════════════════════╪══════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ [ 0.77022531  0.22977469]           │ this 72-minute film does have some exciting scenes , but it's a tad slow .       │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.02438407  0.97561593]           │ an older cad instructs a younger lad in zen and the art of getting laid in this  │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [  8.85042499e-04   9.99114958e-01] │ sandra nettelbeck beautifully orchestrates the transformation of the chilly , ne │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.23184791  0.76815209]           │ [ferrera] has the charisma of a young woman who knows how to hold the screen .   │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.23809737  0.76190263]           │ one of the most exciting action films to come out of china in recent years .     │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.35109395  0.64890605]           │ it offers a glimpse of the solomonic decision facing jewish parents in those tur │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.84233857  0.15766143]           │ it's a road-trip drama with too many wrong turns .                               │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [  3.64625265e-04   9.99635375e-01] │ a quietly reflective and melancholy new zealand film about an eventful summer in │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.07049401  0.92950599]           │ as i settled into my world war ii memories , i found myself strangely moved by e │\n",
      "├─────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [ 0.38802243  0.61197757]           │ a movie that will thrill you , touch you and make you laugh as well .            │\n",
      "╘═════════════════════════════════════╧══════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(\"usando lematizacion sin quitar las stopwords\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "get_model_metrics(features_train_lem2, labels_train_lem2, features_test_lem2,labels_test_lem2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar al no quitar las stop words las metricas para el modelo tambien aumentan, sin embargo el uso de la lematizacion entrega un desempeño un poco menor que el uso de un stemmer esto posiblemente ya que el uso de algoritmos de stemming aunmentan el recall, y el f1 score esta en terminos del recall, por lo que si este aumenta esta metrica el f1 score tambien lo hará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g) Construya una función que entrene/ajuste una Máquina de Vectores de Soporte (SVM) Lineal y mida el error de predicción obtenido sobre los datos de entrenamiento y pruebas. \n",
    "#### * Incluya en su función la exploración de diferentes valores del parámetro de regularización C.\n",
    "#### * Discuta el significado y efecto esperado de este parámetro. Utilice la función construida con los atributos extraídos en el punto (d). \n",
    "#### *  Mida el efecto de filtrar stopwords y de eliminar este paso de pre-procesamiento típico. Determine además, qué representación obtiene un mejor resultado: si aquella obtenida vía lematización o aquella obtenida vía stemming. \n",
    "* Finalmente, tome un subconjunto aleatorio de los textos de prueba y analice las predicciones del modelo (explore las predicciones, así como las probabilidades que el clasificador asigna a cada clase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print(\"C Value:\",C)\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        print(model.score(xt,yt))\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "        print(\"====================================================\")\n",
    "        \n",
    "def do_SVM2(x,y,xt,yt,C):\n",
    "        print(\"C Value:\",C)\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        return model\n",
    "        \n",
    "def get_random_set(features_train,labels_train, features_test, labels_test,C,n):\n",
    "    model = do_SVM2(features_train,labels_train,features_test,labels_test,C)\n",
    "    #Retorna la probabilidad de pertenencia para cada clase del modelo usando el vector de test.\n",
    "    #test_pred = model.decision_function(features_test)\n",
    "    test_pred = model.predict(features_test)\n",
    "    #Selecciona de forma aleatorea la posicion de 15 elementos en el vector test_pred.\n",
    "    spl = random.sample(range(len(test_pred)),n)\n",
    "    data = []\n",
    "    for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "        data.append([sentiment, text[0:85]])\n",
    "    print(tabulate(data, headers=['Sentiment', 'Text'],tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ibm.com/support/knowledgecenter/es/SS3RA7_18.0.0/modeler_mainhelp_client_ddita/clementine/svm_perfimp.html\n",
    "\n",
    "Tenemos que el parametro C de la funcion LinearSVC, corresponde a un parametro de regularizacion, es decir, le dice a la SVM cuánto desea evitar la clasificación errónea de cada dato de entrenamiento. Tenemos que para valores grandes de C, se escogerá un hiperplano de margen más pequeño, esperando que ese hiperplano clasifique mejor los datos del conjunto de entrenamiento. Por el contrario, un valor muy pequeño de C hará que el optimizador busque un hiperplano de separación de mayor margen, incluso si ese hiperplano clasifica erróneamente más puntos. Todo lo anterior se traduce en encontrar el valor de C optimo que le de una mejor comportamiento a nuestro modelo. A continuación  se probará de varias SVM usando Stemming y lematizacion para varios valores de C y filtrando o no las Stop Words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas\n",
    "\n",
    "En esta parte se utilizará lematización y el algoritmo de porter considerando la elieminación de las stopwors o \"palabras vacias\", es decir, todas aquellas palabras que no son relevantes (ya que no poseen significado alguno y que por lo general son los conectores, pronombres, preposiciones y articulos presentes en una oración) como parte del pre-procesamiento de la data para nuestro clasificador. Además para poder determinar que modelo se comporta de mejor manera comprando los f1 score obtenidos a partir de los distintos valores de C.\n",
    "\n",
    "##### Pruebas usando WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.01\n",
      "0.710748452448\n",
      "Training Accuracy SVM: 0.882667\n",
      "Test Accuracy SVM: 0.710667\n",
      "Error: 0.289251547552054\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.73      0.72      1803\n",
      "          -       0.71      0.70      0.70      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 0.1\n",
      "0.719752391671\n",
      "Training Accuracy SVM: 0.987338\n",
      "Test Accuracy SVM: 0.719674\n",
      "Error: 0.28024760832864376\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1\n",
      "0.705402363534\n",
      "Training Accuracy SVM: 0.999156\n",
      "Test Accuracy SVM: 0.705319\n",
      "Error: 0.2945976364659538\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.71      0.71      1803\n",
      "          -       0.70      0.70      0.70      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 10\n",
      "0.692740574001\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.692654\n",
      "Error: 0.3072594259988745\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.70      0.70      0.70      1803\n",
      "          -       0.69      0.69      0.69      1751\n",
      "\n",
      "avg / total       0.69      0.69      0.69      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 100\n",
      "0.685706246483\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.685618\n",
      "Error: 0.31429375351716377\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.68      0.71      0.70      1803\n",
      "          -       0.69      0.66      0.68      1751\n",
      "\n",
      "avg / total       0.69      0.69      0.69      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1000\n",
      "0.679797411367\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.679707\n",
      "Error: 0.3202025886325267\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.67      0.72      0.69      1803\n",
      "          -       0.69      0.64      0.66      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "do_SVM(features_train_lem,labels_train_lem, features_test_lem, labels_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar en las pruebas realizadas, los mejores indicadores se obtienen para un valor de $C = 0.1$, teniendo un training accuracy de 0.987338, un test accuracy de 0.719674 para la data de test y un f1 score de 0.72.\n",
    "\n",
    "Para valores a partir de $C = 1$ el conjunto de entrenamiento tiene un accuray de 1, lo que hace que se produzca sobreajuste, viendose reflejado en que todos los indicadores a partir de este valor comienzan a decaer mientras más grande es el valor de C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas usando PorterStemmer\n",
    "\n",
    "Análogo a lo anterior se realizaran las pruebas sin quitar las stop words , pero esta vez usando stemming a traves del algoritmo de porter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.01\n",
      "0.729319077096\n",
      "Training Accuracy SVM: 0.872538\n",
      "Test Accuracy SVM: 0.729243\n",
      "Error: 0.2706809229037704\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.75      0.74      1803\n",
      "          -       0.73      0.71      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 0.1\n",
      "0.733539673607\n",
      "Training Accuracy SVM: 0.981711\n",
      "Test Accuracy SVM: 0.733465\n",
      "Error: 0.2664603263927969\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1\n",
      "0.720596510974\n",
      "Training Accuracy SVM: 0.999719\n",
      "Test Accuracy SVM: 0.720518\n",
      "Error: 0.27940348902644907\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 10\n",
      "0.705402363534\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.705319\n",
      "Error: 0.2945976364659538\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.69      0.70      1803\n",
      "          -       0.69      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 100\n",
      "0.703151378728\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.703068\n",
      "Error: 0.2968486212718064\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1000\n",
      "0.702588632527\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.702505\n",
      "Error: 0.29741136747326957\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "do_SVM(features_train_porter,labels_train_porter, features_test_porter, labels_test_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso podemos ver que el valor de C que entrega un mejor resultado es $C = 0.1$. Al igual que para el caso, anterior podemos notar que a partir de valores $C = 1$ el training accuracy es practicamente 1, lo que hace que los indicadores para la data de test vaya disminuyendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas sin quitar las stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará lematización y el algoritmo de porter manteniendo las stopwors o \"palabras vacias\", en la parte del pre-procesamiento de la data para nuestro clasificador. Además al igual  que la parte anterior para poder determinar que modelo se comporta de mejor manera se comparará los f1 score obtenidos a partir de los distintos valores de C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruebas usando WordNetLmmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.01\n",
      "0.714124929657\n",
      "Training Accuracy SVM: 0.870850\n",
      "Test Accuracy SVM: 0.714044\n",
      "Error: 0.2858750703432752\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.71      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 0.1\n",
      "0.733258300506\n",
      "Training Accuracy SVM: 0.986775\n",
      "Test Accuracy SVM: 0.733183\n",
      "Error: 0.26674169949352844\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.74      1803\n",
      "          -       0.73      0.74      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1\n",
      "0.714406302757\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.714326\n",
      "Error: 0.28559369724254363\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.70      0.71      1803\n",
      "          -       0.70      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 10\n",
      "0.705402363534\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.705319\n",
      "Error: 0.2945976364659538\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.69      0.70      1803\n",
      "          -       0.69      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 100\n",
      "0.708216094541\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.708134\n",
      "Error: 0.29178390545863814\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.69      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1000\n",
      "0.707934721441\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.707853\n",
      "Error: 0.2920652785593697\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.69      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "do_SVM(features_train_lem2,labels_train_lem2, features_test_lem2, labels_test_lem2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente encontramos que el mejor valor para el parametro de regularización será $C=0.1$ ya que se obtienen mejores metricas en comparación con las demás pruebas para los diferentes valores de este parametro. Además notamos que al mantener las palabras vacias las matricas en general aumentaron especialmente en los valores mayores a $C=0.01$ en comparación con el caso donde estas se eliminaban. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas usando PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.01\n",
      "0.729881823298\n",
      "Training Accuracy SVM: 0.868317\n",
      "Test Accuracy SVM: 0.729806\n",
      "Error: 0.2698368036015757\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.74      0.74      1803\n",
      "          -       0.73      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 0.1\n",
      "0.739729881823\n",
      "Training Accuracy SVM: 0.982836\n",
      "Test Accuracy SVM: 0.739657\n",
      "Error: 0.2602701181767023\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1\n",
      "0.729319077096\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.729243\n",
      "Error: 0.2706809229037704\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.72      0.73      1803\n",
      "          -       0.72      0.74      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 10\n",
      "0.718345526168\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.718266\n",
      "Error: 0.28137310073157007\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.70      0.72      1803\n",
      "          -       0.71      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 100\n",
      "0.714124929657\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.714044\n",
      "Error: 0.28559369724254363\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.70      0.71      1803\n",
      "          -       0.70      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n",
      "C Value: 1000\n",
      "0.712999437254\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.712919\n",
      "Error: 0.2867191896454699\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.70      0.71      1803\n",
      "          -       0.70      0.73      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "do_SVM(features_train_porter2,labels_train_porter2, features_test_porter2, labels_test_porter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nuevamente encontramos que el mejor valor para el parametro de regularización será $C=0.1$ ya que se obtienen mejores metricas en comparación con las demás pruebas para los diferentes valores de este parametro. Además notamos que al mantener las palabras vacias las matricas en general aumentaron $0.01$ en comparación con el caso donde estas se eliminaban. \n",
    "\n",
    "Finalmente al igual que en Naive Bayes, tenemos que al usar Stemming en el modelo se obtiene un mejor resultado, esto posiblemente a que se aumenta el \"ruido\" a la hora de realizar la clasificación dandole una mayor capacidad de generalizacion al modelo. Tambien logramos identificar en todas nuestras pruebas que el valor optimo para el parametro de regularizacion será $C = 0.1$ ya que al usarlo siempre y de forma generalizada se obtenia una mejora para todas metricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.1\n",
      "╒═════════════╤═══════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│   Sentiment │ Text                                                                                  │\n",
      "╞═════════════╪═══════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│           1 │ it's endlessly inventive , consistently intelligent and sickeningly savage .          │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ an escapist confection that's pure entertainment .                                    │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ never quite transcends jokester status . . . and the punchline doesn't live up to bar │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ the artwork is spectacular and unlike most animaton from japan , the characters move  │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ simply and eloquently articulates the tangled feelings of particular new yorkers deep │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ it is intensely personal and yet -- unlike quills -- deftly shows us the temper of th │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ a thought-provoking picture .                                                         │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ o ótimo esforço do diretor acaba sendo frustrado pelo roteiro , que , depois de levar │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ a comedy that is warm , inviting , and surprising .                                   │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ with each of her three protagonists , miller eloquently captures the moment when a wo │\n",
      "╘═════════════╧═══════════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "get_random_set(features_train_porter,labels_train_porter, features_test_porter, labels_test_porter,0.1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 1000\n",
      "╒═════════════╤═══════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│   Sentiment │ Text                                                                                  │\n",
      "╞═════════════╪═══════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│           0 │ demands too much of most viewers .                                                    │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ an escapist confection that's pure entertainment .                                    │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           0 │ they threw loads of money at an idea that should've been so much more even if it was  │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           0 │ well-nigh unendurable . . . though the picture strains to become cinematic poetry , i │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ . . . no charm , no laughs , no fun , no reason to watch .                            │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ . . . stumbles over every cheap trick in the book trying to make the outrage come eve │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           0 │ a sleek advert for youthful anomie that never quite equals the sum of its pretensions │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ it's exactly the kind of movie toback's detractors always accuse him of making .      │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           0 │ it's neither as romantic nor as thrilling as it should be . but it offers plenty to p │\n",
      "├─────────────┼───────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│           1 │ expect no major discoveries , nor any stylish sizzle , but the film sits with square  │\n",
      "╘═════════════╧═══════════════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "get_random_set(features_train_porter,labels_train_porter, features_test_porter, labels_test_porter,1000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
